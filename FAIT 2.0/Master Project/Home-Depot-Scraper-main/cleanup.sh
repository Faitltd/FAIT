#!/bin/bash

# 1. Remove redundant and old files
rm -f building_supplies_cli_scraper.py
rm -f building_supplies_scraper.py
rm -f product_extractor.py
rm -f home_depot_scraper.py
rm -f simple_scraper.py
rm -f fixed_scraper.py
rm -f simple_web_ui.py

# 2. Remove redundant documentation
rm -f runguide.md
rm -f README_WEEKLY_SCRAPING.md
rm -f product_extraction_guide.md

# 3. Consolidate documentation into README.md
cat << 'EOF' >> README.md

## Weekly Scraping

### Running the Weekly Scraper

The scraper can be run in two ways:

1. Using the scheduling script:
```bash
./schedule_weekly_scraper.sh
```

2. Directly using Python:
```bash
python3 improved_weekly_scraper.py
```

### File Locations
All scraper files are organized in the homedepot_scraper package.

### Components
1. Weekly Scraper Script (improved_weekly_scraper.py)
   - Handles category browsing and product detail extraction
   - Generates date-stamped CSV files
   - Saves raw JSON data

2. Scheduler Script (schedule_weekly_scraper.sh)
   - Runs the scraper and logs results
   - Designed for cron job scheduling
EOF

# 4. Create organized directory structure
mkdir -p {scripts,config}

# 5. Move files to appropriate directories
mv improved_weekly_scraper.py scripts/
mv schedule_weekly_scraper.sh scripts/
mv weekly_building_supplies_scraper.py scripts/
mv .env.sample config/

# 6. Make scripts executable
chmod +x scripts/*.sh
chmod +x scripts/*.py

# 7. Clean up data files generated by the scraper
rm -rf data
rm -f *.log
rm -f search_*.json

echo "Cleanup completed successfully!"